{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1HR-eRUa_WeBRXHFpolCEBGXE1IFHx-dm","timestamp":1683648573717}],"gpuType":"T4","authorship_tag":"ABX9TyNkOpy+TNgYAsyJs8g+pqnj"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"ypmHmWG9vjZh","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1683585524871,"user_tz":240,"elapsed":22523,"user":{"displayName":"Leonardo Seoane","userId":"14970497429116975589"}},"outputId":"600da606-f60e-4d60-e42e-cb735f08894e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["dataroot='/content/drive/MyDrive/'\n","# !unzip /content/drive/MyDrive/images_zip.zip -d \"/content/drive/My Drive/585_data\"\n","# !unzip /content/drive/MyDrive/ground_truth_zip.zip -d \"/content/drive/My Drive/585_data\"\n","IS_GPU = True\n","TEST_BS = 2\n","TRAIN_BS = 4"],"metadata":{"id":"3OR-EipUvkCG"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0NTOBjSovWgp"},"outputs":[],"source":["\n","from __future__ import print_function\n","from PIL import Image\n","import os\n","import os.path\n","import numpy as np\n","import sys\n","if sys.version_info[0] == 2:\n","    import cPickle as pickle\n","else:\n","    import pickle\n","\n","from torchvision.datasets.utils import download_url, check_integrity\n","\n","import matplotlib.pyplot as plt\n","from tqdm.notebook import tqdm\n","import torch.utils.data\n","import torchvision\n","import torchvision.transforms as transforms\n","import torchvision.datasets\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.optim.lr_scheduler import StepLR\n","import glob\n","import cv2\n","import torch\n","import torchvision.utils as vutils\n","import torchvision.models as models\n","import torch.optim as optim\n","import scipy.io\n","from torch.utils.data import Dataset\n","\n","os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n","\n","dataroot='/content/drive/MyDrive/585_data/alt_images/'\n","\n","\n","train_transform = transforms.Compose(\n","    [\n","     transforms.CenterCrop((224,224)),\n","     transforms.RandomHorizontalFlip(),\n","     transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),\n","     transforms.ToTensor(),\n","     ])\n","\n","test_transform = transforms.Compose(\n","    [\n","     transforms.CenterCrop((224,224)),\n","     transforms.ToTensor(),\n","    ])\n","\n","\n","class GANDataset(Dataset):\n","    def __init__(self, root_dir, split, transform=None):\n","        self.root_dir = root_dir\n","        self.split = split\n","        self.transform = transform\n","        \n","        self.img_folder = os.path.join(self.root_dir, \"images/\", self.split)\n","        self.gt_folder = os.path.join(self.root_dir, \"ground_truth/\", self.split)\n","  \n","    \n","        self.img_filenames = sorted(os.listdir(self.img_folder))\n","        self.gt_filenames = sorted(os.listdir(self.gt_folder))\n","\n","    def __len__(self):\n","        return len(self.img_filenames)\n","        \n","    def __getitem__(self, idx):\n","        img_path = os.path.join(self.img_folder, self.img_filenames[idx])\n","        gt_path = os.path.join(self.gt_folder, self.gt_filenames[idx])\n","        \n","        img = Image.open(img_path)\n","        gt = Image.open(gt_path).convert('RGB')\n","       \n","        \n","        if self.transform:\n","            img = self.transform(img)\n","            gt=self.transform(gt)\n","        \n","        return img, gt\n","\n","dataset=GANDataset(dataroot,split='train',transform=train_transform)\n","device = torch.device(\"cuda:0\" if (torch.cuda.is_available()) else \"cpu\")\n","\n","\n","dataloader = torch.utils.data.DataLoader(dataset, batch_size=TRAIN_BS,\n","                                         shuffle=True, num_workers=0)\n","\n","val_dataset=GANDataset(dataroot,split='val',transform=train_transform)\n","\n","val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=TEST_BS,\n","                                         shuffle=True, num_workers=0)\n","\n"]},{"cell_type":"code","source":["#from https://github.com/captanlevi/Contour-Detection-Pytorch\n","class Encoder(nn.Module):\n","    def __init__(self, vgg):\n","        super().__init__()\n","        self.vgg = list(vgg.children())\n","        self.vgg=self.vgg[0]\n","        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2, return_indices=True)\n","        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2, return_indices=True)\n","        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2, return_indices=True)\n","        self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2, return_indices=True)\n","        self.pool5 = nn.MaxPool2d(kernel_size=2, stride=2, return_indices=True)\n","        self.conv6 = nn.Conv2d(in_channels = 512, out_channels = 4096, kernel_size = 3, stride=1, padding = 1)\n","    def forward(self,x):\n","\n","        pooling_info = {}\n","        layer_info = {}\n","        # Starting conv1\n","        x = self.vgg[0](x)\n"," \n","        x = self.vgg[1](x)\n","        x = self.vgg[2](x)\n","        x = self.vgg[3](x)\n","        shape = x.shape\n","        \n","        layer_info[1] = {\"value\": x}\n","        x , ind = self.pool1(x)\n","        pooling_info[1] = {\"kernel_size\" : 2, \"stride\": 2, \"padding\": 0 ,\"output_size\": shape,\"indices\":ind}\n","        \n","\n","        # start conv2\n","        x = self.vgg[5](x)\n","        x = self.vgg[6](x)\n","        x = self.vgg[7](x)\n","        x = self.vgg[8](x)\n","\n","        shape = x.shape\n","        layer_info[2] = {\"value\": x}\n","        x , ind = self.pool2(x)\n","        pooling_info[2] = {\"kernel_size\" : 2, \"stride\": 2, \"padding\": 0 ,\"output_size\": shape,\"indices\":ind}\n","\n","\n","\n","        # start conv3\n","        x = self.vgg[10](x)\n","        x = self.vgg[11](x)\n","        x = self.vgg[12](x)\n","        x = self.vgg[13](x)\n","        x = self.vgg[14](x)\n","        x = self.vgg[15](x)\n","\n","        shape = x.shape\n","        layer_info[3] = {\"value\": x}\n","        x , ind = self.pool3(x)\n","        pooling_info[3] = {\"kernel_size\" : 2, \"stride\": 2, \"padding\": 0 ,\"output_size\": shape,\"indices\":ind}\n","  \n","\n","        x = self.vgg[17](x)\n","        x = self.vgg[18](x)\n","        x = self.vgg[19](x)\n","        x = self.vgg[20](x)\n","        x = self.vgg[21](x)\n","        x = self.vgg[22](x)\n","\n","\n","        shape = x.shape\n","        layer_info[4] = {\"value\": x}\n","        x , ind = self.pool4(x)\n","        pooling_info[4] = {\"kernel_size\" : 2, \"stride\": 2, \"padding\": 0 ,\"output_size\": shape,\"indices\":ind}\n","      \n","\n","\n","        x = self.vgg[24](x)\n","        x = self.vgg[25](x)\n","        x = self.vgg[26](x)\n","        x = self.vgg[27](x)\n","        x = self.vgg[28](x)\n","        x = self.vgg[29](x)\n","\n","        shape = x.shape\n","        layer_info[5] = {\"value\": x}\n","        x , ind = self.pool5(x)\n","        pooling_info[5] = {\"kernel_size\" : 2, \"stride\": 2, \"padding\": 0 ,\"output_size\": shape,\"indices\":ind}\n","    \n","        x = self.conv6(x)\n","\n","        \n","        return x , pooling_info, layer_info\n","         \n","\n","class Decoder(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.dconv6 = nn.Conv2d(in_channels = 4096, out_channels = 512, kernel_size = 1, stride=1)\n","       \n","        self.deconv5 = nn.ConvTranspose2d(in_channels = 512, out_channels = 512, kernel_size = 5, padding =2)\n","        self.deconv4 = nn.ConvTranspose2d(in_channels = 512, out_channels = 256, kernel_size = 5 , padding = 2)\n","        self.deconv3 = nn.ConvTranspose2d(in_channels = 256, out_channels = 128, kernel_size = 5 ,padding = 2)\n","        self.deconv2 = nn.ConvTranspose2d(in_channels = 128, out_channels = 64, kernel_size = 5 , padding = 2)\n","        self.deconv1 = nn.ConvTranspose2d(in_channels = 64, out_channels = 32, kernel_size = 5 ,padding = 2)\n","        self.pred = nn.ConvTranspose2d(in_channels = 32, out_channels =3, kernel_size = 5, padding = 2)\n","\n","    def forward(self,encoder_out):\n","        x = encoder_out[0]\n","        dicts = encoder_out[1]\n","\n","\n","        x = self.dconv6(x)\n","        x = nn.functional.relu(x)\n","        x = nn.functional.max_unpool2d(x, **dicts[5])\n","\n","        \n","     \n","        x = self.deconv5(x)\n","        x = nn.functional.relu(x)\n","        x = nn.functional.max_unpool2d(x, **dicts[4])  # Indices 512\n","\n","\n","        x = self.deconv4(x)\n","        x = nn.functional.relu(x)\n","        x = nn.functional.max_unpool2d(x, **dicts[3])  # Indices 256\n","\n","\n","        x = self.deconv3(x)\n","        x = nn.functional.relu(x)\n","        x = nn.functional.max_unpool2d(x, **dicts[2])  # Indices 128\n","\n","\n","        x = self.deconv2(x)\n","        x = nn.functional.relu(x)\n","        x = nn.functional.max_unpool2d(x, **dicts[1])  # Indices 64\n","        x = self.deconv1(x)\n","        x = nn.functional.relu(x)\n","\n","        x = self.pred(x)\n","\n","        x = torch.sigmoid(x)\n","        return x\n","    \n","class countour_detector(nn.Module):\n","    def __init__(self, backbone):\n","        super().__init__()\n","        self.encoder = Encoder(backbone.to(device)).to(device)\n","        self.decoder = Decoder().to(device)\n","\n","    def forward(self,x):\n","        x = self.encoder(x)\n","        return self.decoder(x)\n","\n","\n","vgg16 =  torchvision.models.vgg16(pretrained=True).to(device)\n","\n","for p in vgg16.parameters():\n","    p.requires_grad = False\n"],"metadata":{"id":"UnuOvMXzwueY","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1683585568861,"user_tz":240,"elapsed":9263,"user":{"displayName":"Leonardo Seoane","userId":"14970497429116975589"}},"outputId":"a3270204-e316-4f45-a6de-7a00bbd361ce"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n","Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n","100%|██████████| 528M/528M [00:01<00:00, 310MB/s]\n"]}]},{"cell_type":"code","source":["#From PS2 template, and from https://github.com/captanlevi/Contour-Detection-Pytorch\n","generator = countour_detector(backbone=vgg16).to(device)\n","optimizer = torch.optim.Adam([x for x in list(generator.parameters()) if x.requires_grad == True], lr=.0001,weight_decay=1e-4)\n","def context_loss(outputs, targets):\n","        weights = torch.empty_like(targets).to(device)\n","        weights[targets >= .98] = 10\n","        weights[targets < .98] = 1\n","        loss = F.binary_cross_entropy(outputs, targets, weights)\n","        return loss \n","\n","best_loss = float('inf')\n","num_epochs = 15\n","best_model_state_dict = None\n","for epoch in range(num_epochs):\n","    running_loss=0\n","    for batch_idx, (real_samples, labels) in enumerate(dataloader):\n","        real_samples=real_samples.to(device)\n","        labels=labels.to(device)\n","        # Train the generator\n","        generator.zero_grad()\n","        output=generator(real_samples)\n","        g_loss=context_loss(output,labels)\n","        g_loss.backward()\n","        optimizer.step()\n","        running_loss += g_loss.item()\n","    \n","  \n","        if batch_idx % 10 == 0:\n","            print(\"Epoch [{}/{}], Batch [{}/{}],  G_loss: {:.4f}\".format(\n","                epoch+1, num_epochs, batch_idx+1, len(dataloader), g_loss.item()))\n","        \n","        if batch_idx % 200==0:\n","          with torch.no_grad():\n","                val_loss = 0\n","                for batch_idx, (data, target) in enumerate(val_dataloader):\n","                    # print(len(val_dataloader))\n","                    data=data.to(device)\n","                    target=target.to(device)\n","                    output = generator(data)\n","                    val_loss += context_loss(output, target)\n","                  \n","                val_loss /= len(val_dataloader)\n","                print(val_loss)\n","                if val_loss < best_loss:\n","                      best_loss = val_loss\n","                      best_model_state_dict = generator.state_dict()\n","                      torch.save(best_model_state_dict, dataroot+'best_enc_dec_model.pt')\n","            \n"],"metadata":{"id":"KgI1-A98w101","colab":{"base_uri":"https://localhost:8080/","height":381},"executionInfo":{"status":"error","timestamp":1683585570885,"user_tz":240,"elapsed":2026,"user":{"displayName":"Leonardo Seoane","userId":"14970497429116975589"}},"outputId":"25dd1c72-19fd-4c03-d416-bbed46c24179"},"execution_count":null,"outputs":[{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-6-1a20a8f1f73c>\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mrunning_loss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mreal_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0mreal_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreal_samples\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    632\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 634\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    635\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    636\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    676\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 678\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    679\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-4-7173fc16ac67>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m         \u001b[0mgt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgt_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'RGB'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   2982\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2983\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2984\u001b[0;31m     \u001b[0mprefix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2985\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2986\u001b[0m     \u001b[0mpreinit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","source":["my_model = countour_detector(backbone=vgg16).to(device)\n","my_model.load_state_dict(torch.load(dataroot+'best_enc_dec_model.pt'))\n","\n","def loss(outputs, targets):\n","        weights = torch.empty_like(targets).to(device)\n","        weights[targets >= .98] = 10\n","        weights[targets < .98] = 1\n","        loss = F.binary_cross_entropy(outputs, targets, weights)\n","        return loss \n","\n","\n","def simple_predict(model):\n","    model.eval()\n","\n","    dataset = GANDataset(dataroot, split='test', transform=train_transform)\n","    dataloader = torch.utils.data.DataLoader(dataset, batch_size=4,\n","                                             shuffle=False, num_workers=0)\n","\n","    with torch.no_grad():\n","        for batch_idx, (real_samples, labels) in enumerate(dataloader):\n","            if batch_idx >1:\n","                break\n","            else:\n","                real_samples = real_samples.to(device)\n","                output = model(real_samples)\n","\n","                real_samples=real_samples.cpu().detach().numpy()[0]\n","                output = output.cpu().detach().numpy()[0]\n","                label = labels.cpu().detach().numpy()[0]\n","                \n","                fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n","                ax1.axis(\"off\")\n","                ax1.set_title(\"Image\")\n","                ax1.imshow(real_samples.transpose((1, 2, 0)))\n","\n","                ax2.axis(\"off\")\n","                ax2.set_title(\"Model Output\")\n","                ax2.imshow(output.transpose((1, 2, 0)))\n","\n","                # print(g_loss)\n","                plt.show()\n","\n","                \n","simple_predict(my_model)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":235},"id":"pJGuVaWenq3J","executionInfo":{"status":"error","timestamp":1683664759512,"user_tz":240,"elapsed":167,"user":{"displayName":"Leonardo Seoane","userId":"14970497429116975589"}},"outputId":"ea2b2a0f-4d4c-4d94-8357-4e546752c5f6"},"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-a9560064dc6d>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmy_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcountour_detector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbackbone\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvgg16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# load the saved model from a file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmy_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataroot\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'best_enc_dec_model.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'countour_detector' is not defined"]}]},{"cell_type":"code","source":["my_model = countour_detector(backbone=vgg16).to(device)\n","my_model.load_state_dict(torch.load(dataroot+'best_enc_dec_model.pt'))\n","\n","              \n","def avg_test_loss(model):\n","    model.eval()\n","    test_loss = 0\n","    num_samples = 0\n","    \n","    dataset = GANDataset(dataroot, split='test', transform=train_transform)\n","    dataloader = torch.utils.data.DataLoader(dataset, batch_size=1,\n","                                             shuffle=True, num_workers=0)\n","\n","    with torch.no_grad():\n","        for batch_idx, (real_samples, labels) in enumerate(dataloader):\n","            real_samples = real_samples.to(device)\n","            labels = labels.to(device)\n","            output = model(real_samples)\n","            batch_loss = loss(output, labels)\n","            test_loss += batch_loss.item()\n","            num_samples += 1\n","\n","    avg_loss = test_loss / num_samples\n","    return avg_loss\n","\n","avg_test_loss(my_model)"],"metadata":{"id":"CDUu1oKl5ZjP"},"execution_count":null,"outputs":[]}]}